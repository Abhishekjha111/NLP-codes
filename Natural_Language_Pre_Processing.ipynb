{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Pre - Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-d8PNd4X_NA",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1600/1*p_zgFaUyb66IeyHsi15soA.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPJSIxy9YHDT",
        "colab_type": "text"
      },
      "source": [
        "**NLP** is short for **Natural Language Processing**. As you probably know, computers are not as great at understanding words as they are numbers. This is all changing though as advances in NLP are happening everyday. The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP. The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language.\n",
        "\n",
        "\n",
        "As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things. Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc. From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zleUuQnJc0eK",
        "colab_type": "text"
      },
      "source": [
        "# NLTK (Natural Language Toolkit)\n",
        "\n",
        "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. Although NLTK has adapted to more than 38 languages at present.\n",
        "\n",
        "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning and wrappers for industrial-strength NLP libraries.  NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIpt5iUxd2e7",
        "colab_type": "text"
      },
      "source": [
        "## NLP Library\t\n",
        "\n",
        "**NLTK** :\tThis is one of the most usable and mother of all NLP libraries.\n",
        "\n",
        "**spaCy**:\tThis is completely optimized and highly accurate library widely used in deep learning\n",
        "\n",
        "**Stanford CoreNLP Python**:\tFor client-server based architecture this is a good library in NLTK. This is written in JAVA, but it provides modularity to use it in Python.\n",
        "\n",
        "**TextBlob**:\tThis is an NLP library which works in python2 and python3. This is used for processing textual data and provide mainly all type of operation in the form of API.\n",
        "\n",
        "**Gensim**:\tGenism is a robust open source NLP library support in python. This library is highly efficient and scalable.\n",
        "\n",
        "**Pattern**:\tIt is a light-weighted NLP module. This is generally used in Web-mining, crawling or such type of spidering task. \n",
        "\n",
        "**Polyglot**:\tFor massive multilingual applications, Polyglot is best suitable NLP library. Feature extraction in the way on Identity and Entity.\n",
        "\n",
        "**PyNLPl**:\tPyNLPI also was known as 'Pineapple' and supports Python. It provides a parser for many data format like FoLiA/Giza/Moses/ARPA/Timbl/CQL.\n",
        "\n",
        "**Vocabulary**:\tThis library is best to get Semantic type information from the given text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oM787FOXrBa",
        "colab_type": "code",
        "outputId": "d3d6c481-e173-4a87-e80b-677e6a81d39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_FQQfXYizDA",
        "colab_type": "code",
        "outputId": "7828c0b3-385e-47ad-9b89-9746150747e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbr-JNS0jGO-",
        "colab_type": "code",
        "outputId": "92e5789c-1fd6-4f02-a526-5178e8df2806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib9mnxTojboi",
        "colab_type": "code",
        "outputId": "d68cb65e-f225-49bd-a764-50a8ee05e595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('genesis')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8CLkw2DkbZn",
        "colab_type": "code",
        "outputId": "3d048bbf-f81a-49f1-aab2-cd320932e6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAap0A7vnEcJ",
        "colab_type": "code",
        "outputId": "5556de56-da30-4561-a5fc-6e5fbc17d5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "whitman = nltk.corpus.gutenberg.words('whitman-leaves.txt')\n",
        "print(whitman)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bahL29ECsBFQ",
        "colab_type": "text"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLTujHgRsEmh",
        "colab_type": "text"
      },
      "source": [
        "We will talk about the basic steps of text preprocessing. These steps are needed for transferring text from human language to machine-readable format for further processing. We will also discuss text preprocessing tools.\n",
        "\n",
        "After a text is obtained, we start with text normalization. Text normalization includes:\n",
        "\n",
        "\n",
        "\n",
        "*    removing punctuations, accent marks and other diacritics\n",
        "*    removing white spaces\n",
        "*    expanding abbreviations\n",
        "*    removing stop words, sparse terms, and particular words\n",
        "*    text canonicalization\n",
        "*    converting all letters to lower or upper case\n",
        "*    converting numbers into words or removing numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQkmTZqyUQx",
        "colab_type": "text"
      },
      "source": [
        "### Removing punctuations, accent marks, special symbols and diacritics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7IMxZmryTxk",
        "colab_type": "code",
        "outputId": "9f6ee311-f2fb-4d70-97e1-e5c84ce4cbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Sample code to remove a regex pattern \n",
        "import re \n",
        "\n",
        "def remove_regex(input_text, regex_pattern):\n",
        "    urls = re.finditer(regex_pattern, input_text) \n",
        "    for i in urls: \n",
        "        input_text = re.sub(i.group().strip(), '', input_text)\n",
        "    return input_text\n",
        "\n",
        "regex_pattern = \"#[\\w]*\"  \n",
        "\n",
        "remove_regex(\"remove this #hashtag from my given string object\", regex_pattern)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'remove this  from my given string object'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dwlWziP4nZR",
        "colab_type": "text"
      },
      "source": [
        "### Remove whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTICTDEB41e0",
        "colab_type": "code",
        "outputId": "31fecf2e-4222-4ce0-8643-b5b710606750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_str = \" \\t a string example\\t \"\n",
        "input_str = input_str.strip()\n",
        "input_str"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a string example'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHPBW4PZ5Eqa",
        "colab_type": "text"
      },
      "source": [
        "### Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4A18T2f5G-E",
        "colab_type": "code",
        "outputId": "528e3ef5-e9bf-4d7b-a4e3-e36f5d7bbf1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
        "result = re.sub(r\"\\d+\", \"\", input_str)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPMpcgsLzJLx",
        "colab_type": "text"
      },
      "source": [
        "{3: 'three'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xo4yDnL5c7x",
        "colab_type": "text"
      },
      "source": [
        "### Convert Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeQIa5xK5gV3",
        "colab_type": "code",
        "outputId": "f520a036-65e3-4a16-916f-43600f924b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = input_str.lower()\n",
        "print(input_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3nA6fDk5toc",
        "colab_type": "text"
      },
      "source": [
        "**Tokenization**\n",
        "\n",
        "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrQ32n096_GA",
        "colab_type": "code",
        "outputId": "220baacc-7b9d-4eca-c98e-5b9812ba3c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAhXTfte5wPN",
        "colab_type": "code",
        "outputId": "c4acdb6b-371f-469e-9614-2a8c135635b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(input_str)\n",
        "print (tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhq0zde57Itt",
        "colab_type": "text"
      },
      "source": [
        "### Remove stop words\n",
        "\n",
        "“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oD8Gnon73KT",
        "colab_type": "code",
        "outputId": "88d243bc-fa10-473e-e6ad-f8d7a341e2bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YIeqnHZ7PUe",
        "colab_type": "code",
        "outputId": "5e50110f-204d-4a81-a243-b5b625bf1db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'did', 'while', 'of', 'were', \"mightn't\", 'wasn', 'herself', 'd', 'under', 'm', 'what', 'as', 'isn', 'y', 'yourselves', 'she', 'that', 'ain', 'where', 'am', 'nor', 'there', 'has', 'own', 'if', 'have', 'why', 'shan', \"aren't\", \"you're\", 'some', \"weren't\", 'this', 'too', 'haven', 'few', 'won', 'again', 'on', 'to', 'through', 'over', 'until', 'by', 're', 'themselves', 'do', 'up', \"mustn't\", \"don't\", 'hadn', 'you', \"won't\", 't', 'yourself', 'when', 'himself', \"doesn't\", 'didn', \"hadn't\", \"hasn't\", 'having', 'ourselves', 'both', 'more', 'them', 'had', 'before', 'hers', \"it's\", \"that'll\", 'very', 'does', 'all', 'in', \"she's\", 'their', 'should', 'mustn', \"you'll\", 'below', 'which', 'my', 'whom', 'it', 'its', 'him', 'other', 'only', 'ours', 'don', 'doing', \"wouldn't\", 'shouldn', 'and', 'is', \"couldn't\", 'hasn', 'couldn', 'into', \"shouldn't\", 'being', 'o', 'can', 'now', 'so', 'we', 'down', 'after', 'are', 'was', 'll', 'but', 'her', 've', \"you've\", 'for', \"wasn't\", 'between', 'who', 'off', 'yours', 'itself', 'those', 'been', 'he', 'no', 'the', 'most', 'about', 'our', 'be', \"didn't\", 'his', 'these', 'here', 'such', 'just', 'a', 'myself', 'me', \"should've\", 'then', 'mightn', 'ma', \"haven't\", \"shan't\", 'from', 'they', 'during', 'how', 'needn', 'an', 'wouldn', 'at', 'further', 'because', 'will', 'theirs', 'than', 'aren', 'not', 'same', \"you'd\", 'doesn', 'above', 'once', 'any', 's', 'out', \"needn't\", 'weren', 'each', 'your', 'with', \"isn't\", 'i', 'or', 'against'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CP3gK7j7-FL",
        "colab_type": "code",
        "outputId": "37bcff5c-1b86-4ff7-f7df-c085b236b34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_str = \"All work and no play makes jack dull boy. Its good to go out and have fun at times.\"\n",
        "tokens = word_tokenize(input_str)\n",
        "result = [i for i in tokens if not i in stop_words]\n",
        "print (result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'Its', 'good', 'go', 'fun', 'times', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvCGoQyH8YSX",
        "colab_type": "code",
        "outputId": "1fdfe6a3-699b-4300-e219-8550efc28974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#sklearn can also provide a list of standard english stop words\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "print (ENGLISH_STOP_WORDS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozenset({'nobody', 'everything', 'around', 'move', 'while', 'of', 'were', 'latter', 'towards', 'without', 'whereupon', 'sometimes', 'herself', 'neither', 'under', 'part', 'whereafter', 'meanwhile', 'sincere', 'as', 'front', 'what', 'detail', 'system', 'she', 'that', 'top', 'yourselves', 'where', 'am', 'nor', 'seem', 'there', 'anything', 'would', 'has', 'may', 'own', 'became', 'if', 'alone', 'have', 'seemed', 'why', 'done', 'third', 'some', 'others', 'this', 'either', 'eight', 'de', 'too', 'few', 'since', 'former', 'describe', 'again', 'ten', 'always', 'on', 'therefore', 'might', 'to', 'last', 'through', 'whole', 'upon', 'hereby', 'over', 'until', 'by', 'whereas', 'behind', 'please', 're', 'do', 'four', 'themselves', 'inc', 'up', 'someone', 'you', 'could', 'two', 'well', 'become', 'anyone', 'yourself', 'cannot', 'due', 'when', 'himself', 'whoever', 'anyway', 'empty', 'hundred', 'none', 'perhaps', 'several', 'amongst', 'these', 'thick', 'ourselves', 'elsewhere', 'both', 'more', 'nowhere', 'show', 'wherever', 'give', 'although', 'had', 'them', 'throughout', 'before', 'hers', 'eg', 'very', 'across', 'first', 'whither', 'besides', 'except', 'mine', 'all', 'in', 'nine', 'should', 'their', 'thence', 'almost', 'below', 'con', 'which', 'my', 'whom', 'thru', 'also', 'etc', 'it', 'its', 'him', 'other', 'only', 'becoming', 'least', 'ours', 'latterly', 'herein', 'twenty', 'back', 'and', 'thereby', 'twelve', 'formerly', 'among', 'is', 'rather', 'fire', 'thus', 'put', 'three', 'name', 'moreover', 'into', 'nothing', 'take', 'un', 'yet', 'indeed', 'must', 'interest', 'being', 'see', 'sixty', 'whatever', 'can', 'nevertheless', 'bill', 'forty', 'us', 'hence', 'hereupon', 'keep', 'another', 'full', 'now', 'side', 'so', 'we', 'whence', 'somewhere', 'anywhere', 'down', 'ever', 'after', 'hereafter', 'are', 'enough', 'was', 'noone', 'thereupon', 'whose', 'but', 'everywhere', 'her', 'seems', 'already', 'for', 'between', 'along', 'whenever', 'who', 'off', 'get', 'go', 'couldnt', 'itself', 'something', 'fifty', 'six', 'otherwise', 'every', 'those', 'everyone', 'becomes', 'hasnt', 'been', 'sometime', 'he', 'mill', 'no', 'the', 'beyond', 'ie', 'about', 'most', 'our', 'per', 'many', 'seeming', 'still', 'wherein', 'be', 'fill', 'his', 'here', 'bottom', 'never', 'co', 'a', 'myself', 'me', 'call', 'onto', 'such', 'then', 'somehow', 'within', 'via', 'less', 'find', 'together', 'from', 'they', 'much', 'during', 'how', 'one', 'an', 'at', 'further', 'cant', 'because', 'cry', 'beside', 'made', 'therein', 'will', 'than', 'fifteen', 'namely', 'not', 'amount', 'toward', 'next', 'thin', 'amoungst', 'thereafter', 'mostly', 'same', 'whether', 'whereby', 'eleven', 'even', 'above', 'serious', 'any', 'once', 'beforehand', 'often', 'ltd', 'out', 'afterwards', 'though', 'yours', 'five', 'found', 'however', 'each', 'your', 'with', 'i', 'else', 'or', 'against', 'anyhow'})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCT4lmDA8oPc",
        "colab_type": "text"
      },
      "source": [
        "Most of what we are going to do with language relies on ﬁrst separating out or tokenizing words (splitting the text into minimal meaningful units) from running text, known as the task of tokenization.\n",
        "\n",
        "English words are often separated from each other by whitespace, but whitespace is not always sufﬁcient. “New York” and “rock ’n’ roll” are sometimes treated as large words despite the fact that they contain spaces, while sometimes we’ll need to separate “I’m” into the two words I and am.\n",
        "\n",
        "For processing tweets or texts we’ll need to tokenize emoticons like “ :)” or hashtags like #nlproc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEDpRmdek2tG",
        "colab_type": "text"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
        "\n",
        "Stemming is a part of linguistic studies in morphology and artificial intelligence (AI) information retrieval and extraction. Stemming and AI knowledge extract meaningful information from vast sources like big data or the Internet since additional forms of a word related to a subject may need to be searched to get the best results. Stemming is also a part of queries and Internet search engines.\n",
        "\n",
        "Recognizing, searching and retrieving more forms of words returns more results. When a form of a word is recognized it can make it possible to return search results that otherwise might have been missed. That additional information retrieved is why stemming is integral to search queries and information retrieval.\n",
        "\n",
        "\n",
        "Applications of stemming are:\n",
        "\n",
        "* Stemming is used in information retrieval systems like search engines.\n",
        "* It is used to determine domain vocabularies in domain analysis.\n",
        "* Stemming is desirable as it may reduce redundancy as most of the time the word stem and their inflected/derived words mean the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81cWs5E0mySu",
        "colab_type": "code",
        "outputId": "4596fc24-7754-4072-b8d9-533f5184c66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer= PorterStemmer()\n",
        "input_str=\"There are several types of stemming algorithms for Natural languages\"\n",
        "input_str=word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(stemmer.stem(word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            "for\n",
            "natur\n",
            "languag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqHDNKw9nbNN",
        "colab_type": "text"
      },
      "source": [
        "**Errors in Stemming**:\n",
        "There are mainly two errors in stemming – Overstemming and Understemming. Overstemming occurs when two words are stemmed to same root that are of different stems. Under-stemming occurs when two words are stemmed to same root that are not of different stems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0zuqyFen0Yl",
        "colab_type": "code",
        "outputId": "4b1eac46-4cae-4b3c-82e7-4ea0ba0bef27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "print(stemmer.stem(\"having\"))\n",
        "print(stemmer2.stem(\"having\"))\n",
        "\n",
        "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
        "\n",
        "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have\n",
            "having\n",
            "generous\n",
            "gener\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmzAmVUQuvIC",
        "colab_type": "text"
      },
      "source": [
        "**N-Gram Stemmer**\n",
        "\n",
        "An n-gram is a set of n consecutive characters extracted from a word in which similar words will have a high proportion of n-grams in common.\n",
        "Example: ‘INTRODUCTIONS’ for n=2 becomes : *I, IN, NT, TR, RO, OD, DU, UC, CT, TI, IO, ON, NS, S*\n",
        "\n",
        "Advantage: It is based on string comparisons and it is language dependent.\n",
        "\n",
        "Limitation: It requires space to create and index the n-grams and it is not time efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHCYQ6GDvP4z",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatizer\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
        "\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEYVaVrVv39A",
        "colab_type": "text"
      },
      "source": [
        "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
        "\n",
        "For instance:\n",
        "\n",
        "The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
        "\n",
        "The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
        "\n",
        "The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrTwcJaEvX0k",
        "colab_type": "code",
        "outputId": "6ec8de9b-e8e8-4c24-e501-50be9d944446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnqi93NnvR1U",
        "colab_type": "code",
        "outputId": "d476715a-270a-42db-b0d4-5522cda0eb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "lemma.lemmatize('played')\n",
        "# lemma.lemmatize('leaves')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'played'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re74ahJWwTf-",
        "colab_type": "text"
      },
      "source": [
        "**Object Standardization**\n",
        "\n",
        "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
        "\n",
        "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mLgigvpwVeU",
        "colab_type": "code",
        "outputId": "50e529f2-6566-41b9-f73d-70729b23fc1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
        "def lookup_words(input_text):\n",
        "    words = input_text.split() \n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "        if word.lower() in lookup_dict:\n",
        "            word = lookup_dict[word.lower()]\n",
        "        new_words.append(word) \n",
        "        new_text = \" \".join(new_words) \n",
        "    return new_text\n",
        "\n",
        "print(lookup_words(\"RT We are going to CCD @ MG Road!! dm for more info.!!\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Retweet We are going to CCD @ MG Road!! direct message for more info.!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}